{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Gradient Descent with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first notebook, we walked through the basics of gradient descent and applied them to linear regression. In this notebook, we will be doing the same thing, but for a new task.\n",
    "\n",
    "### By the end of this lession you will: \n",
    "\n",
    "- Understand the fundamentals of logistic regression\n",
    "- Be able to implement your own model class that's just like sci-kit learns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A new dataset, a new task\n",
    "\n",
    "Now that we've seen one task, regression, it's time to see if we can apply our understanding of Gradient Descent to another common machine learning task: classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "This new dataset is another classic machine learning dataset from the UCI ML datasets for classifying Benign and Malignant [cancer from tissue samples](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic).\n",
    "\n",
    "The goal is to see if we can accurately estimate likelihood of having cancer, given measurements of tissue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(load_breast_cancer().DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_dataset_train_test():\n",
    "    \n",
    "    dataset_object = load_breast_cancer()\n",
    "    X_features_df = pd.DataFrame(data=dataset_object['data'], \n",
    "                                        columns=dataset_object['feature_names'])\n",
    "    y_labels_df = pd.DataFrame(data=dataset_object['target'], \n",
    "                               columns=['target_labels'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features_df, y_labels_df, shuffle=True, test_size=0.2)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_classification_dataset_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the outcome variable against one of our input features.\n",
    "\n",
    "---\n",
    "**_Note_**: I have flipped the sign. In our dataset $1 = {Malignan}t$ and $0 = {Benign}$, but I have flipped it for convinience of plotting such that $0 = {Malignant}$ and $1 = {Benign}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes()\n",
    "plt.scatter(X_train['mean radius'], 1 - y_train['target_labels'], color='darkorange')\n",
    "ax.set(xlabel=\"X Input\", ylabel=\"Cancer, 'Benign' = 1\",\n",
    "       title=\"'Benign' or 'Malignant' tissue given some input\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does our simple linear regression model work?\n",
    "\n",
    "If we wanted to predict the outcome, 0 or 1, could we use our simple linear regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes()\n",
    "plt.scatter(X_train['mean radius'], 1 - y_train['target_labels'], color='darkorange')\n",
    "plt.plot([10, 20], [-0.1, 1.1], color='black')\n",
    "ax.set(xlabel=\"X Input\", ylabel=\"Cancer, 'Benign' = 1\",\n",
    "       title=\"'Benign' or 'Malignant' tissue given some input\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looks like we need a new model\n",
    "\n",
    "We need a new model that will help map from inputs to, in this case, a binary output bounded by 0 and 1. We can no longer use _just_ a model that is a straight line, ie. our trusty $Y_{pred} = \\sum_i^j(X_j*W_j)$, but we need a new model that can, ideally, take that _weighted sum_ and then *squashes* it in between $0 <= Y_{pred} <= 1$\n",
    "\n",
    "\n",
    "## Logistic regression, and the sigmoid, to the rescue! \n",
    "\n",
    "If we take our weighted sum: $\\sum_i^j(X_j*W_j) = dot(X, W)$, but pass it through another function called the **_sigmoid_**, it will weight our inputs just like a linear regression, but magically map them to 0 and 1. \n",
    "\n",
    "$$ \\frac{1}{1 + e^{-dot(X,W)} } $$\n",
    "\n",
    "This helps map our inputs to something that acts like a **_probability_**! Now, we have a model that will take inputs, weight them, and then return us a likelihood of the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1 / (1. + np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes()\n",
    "X = np.linspace(8, 23, 50)\n",
    "y = [sigmoid(x - 14) for x in X]\n",
    "plt.plot(X, y, color='black')\n",
    "ax.set(xlabel=\"X Input\", ylabel=\"Probability of Cancer, 'Benign' = 1\",\n",
    "       title=\"Probability mapped of 'Benign' or 'Malignant' tissue given some input\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes()\n",
    "plt.scatter(X_train['mean radius'], 1 - y_train['target_labels'], color='darkorange')\n",
    "X = np.linspace(8, 23, 50)\n",
    "y = [sigmoid(x - 14) for x in X]\n",
    "plt.plot(X, y, color='black')\n",
    "ax.set(xlabel=\"X Input\", ylabel=\"Probability of Cancer, 'Benign' = 1\",\n",
    "       title=\"Probability mapped of 'Benign' or 'Malignant' tissue given some input\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once again, finding the optimal weight\n",
    "\n",
    "We're left with a similar problem: given the data we have, what is the optimal weight that we should choose? \n",
    "\n",
    "However, given this new model, we need a new definition or criteria for measuring \"wrong-ness\" or error. Without going to deep into the details, there exists another mathematical tool for estimating how optimally close probabilities are to their associated outcomes called: **_cross entropy_**. It's often called the **_log loss_** (sounds an aweful lot like our \"_Logistic_ regression\"?!) \n",
    "\n",
    "$$ Log Loss := -\\frac{1}{N} \\sum_i^N \\sum_j^M(Y_{ij}*\\log{(P_{ij})}) $$ \n",
    "\n",
    "For $N$ examples of $M$ classes.\n",
    "\n",
    "For the binary case, it's simply:\n",
    "\n",
    "$$ Binary Log Loss := -\\frac{1}{N} \\sum_i^N(Y_{i}*\\log{(P_{i})} + (1 - Y_{i})*\\log{(1 - P_{i})}) $$ \n",
    "\n",
    "And, as you can see from above, our $P_{i} = Sigmoid(X,W)$ because our sigmoid is our handy tool for producing probabilities from weighted sums of inputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take another look at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal for the rest of the session\n",
    "\n",
    "Just as we have done for the Linear Regression, let's implement our own model called `MyLogisticRegression` as a python `class` that can `fit()` and learn the optimal weights and be used to predict the output probability.\n",
    "\n",
    "\n",
    "Take a look at the `models.py` file and you'll see that you have a class to implement. Once you've properly implemented this, you should have a working model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your input features here!\n",
    "feature_cols = ['mean radius', 'mean texture', 'mean compactness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MyLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = MyLogisticRegression(learning_rate=0.01, max_iterations=100000, optimizer='simple_gd', epochs=2500, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_model.fit(X_train[feature_cols].to_numpy(), y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once again, let's comnpare to the _pros_\n",
    "\n",
    "Take a look at your new model class as it compares to the implementations in Sklearn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model = LogisticRegression(penalty='none', verbose=2, solver='saga', fit_intercept=True, max_iter=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model.fit(X_train[feature_cols].to_numpy(), y_train.to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn_model.intercept_, sklearn_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, another example of a similar model that is implementing a different flavor of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_sgd = SGDClassifier(loss='log', penalty='none', alpha=0.0, max_iter=100000, verbose=2, learning_rate='constant', eta0=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sklearn_sgd.fit(X_train[feature_cols].to_numpy(), y_train.to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn_sgd.intercept_, sklearn_sgd.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After looking at those coefficients, look at the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_test.to_numpy().ravel(), test_model.predict_probability(X_test[feature_cols].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_test.to_numpy().ravel(), sklearn_model.predict_proba(X_test[feature_cols].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_test.to_numpy().ravel(), sklearn_sgd.predict_proba(X_test[feature_cols].to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing notes\n",
    "\n",
    "Now you have your own machine learning model that implements logistic regression with Gradient or Stochastic Gradient Descent!\n",
    "\n",
    "There are many areas of the subject that we **did not** discuss that are critical to deploying gradient descent. Some further reading would be:\n",
    "\n",
    "- Advanced Stochastic Gradient Descent\n",
    "- Momentum and variants on Stochastic Gradient Descent\n",
    "- The importance of _scaled_ inputs for Gradient Descent (having weights that are very large and very small depending on the values)\n",
    "- Early stopping techniques\n",
    "- Initialization techniques\n",
    "\n",
    "There's so much more to discover! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
